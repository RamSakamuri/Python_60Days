{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RamSakamuri/Python_60Days/blob/main/1_Text_Representation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tS27LV3yP0Ab"
      },
      "source": [
        "# Introduciton to NLP\n",
        "\n",
        "NLP is a subfield of computer science and artificial intelligence concerned with interactions between computers and human (natural) languages. It is used to apply machine learning algorithms to text and speech.\n",
        "For example, we can use NLP to create systems like speech recognition, document summarization, machine translation, spam detection, named entity recognition, question answering, autocomplete, predictive typing and so on.\n",
        "Nowadays, most of us have smartphones that have speech recognition. These smartphones use NLP to understand what is said. Also, many people use laptops which operating system has a built-in speech recognition.\n",
        "\n",
        "\n",
        "# Libraries\n",
        "\n",
        "If you are familiar with python you know there is at least one library for everything in NLP ther are many!\n",
        "\n",
        "## NLTK\n",
        "![alt text](https://i2.wp.com/clay-atlas.com/wp-content/uploads/2019/08/python_nltk.png)\n",
        "\n",
        "NLTK is one of the leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries.\n",
        "\n",
        "It's very popular in accademia nad implements many different algorithms, but lacks more modern approaces \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKRNYZSreh1H",
        "outputId": "50af55db-ff04-428c-8543-ed5cf102041d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.10/dist-packages (0.7.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.5.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.65.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.7)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.22.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.27.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.22.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.3.0)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.10.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.28.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacymoji in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: emoji<1.0.0,>=0.4.5 in /usr/local/lib/python3.10/dist-packages (from spacymoji) (0.6.0)\n",
            "Requirement already satisfied: spacy<4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from spacymoji) (3.5.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (2.0.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (23.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (3.0.12)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (4.65.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (2.4.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (3.3.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (3.0.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (3.1.2)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (1.1.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (2.0.8)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (1.22.4)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (0.7.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (1.0.9)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (0.10.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (2.27.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (8.1.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (67.7.2)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (1.0.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (1.10.7)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<4.0.0,>=3.0.0->spacymoji) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacymoji) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacymoji) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacymoji) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacymoji) (3.4)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.0.0->spacymoji) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.0.0->spacymoji) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacymoji) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<4.0.0,>=3.0.0->spacymoji) (2.1.2)\n"
          ]
        }
      ],
      "source": [
        "# !pip install nltk\n",
        "# !pip install pyspellchecker\n",
        "# !pip install spacy\n",
        "# !pip install gensim\n",
        "# !pip install transformers\n",
        "# !pip install spacymoji\n",
        "\n",
        "import nltk\n",
        "# In your local machine you will have a nice interface\n",
        "nltk.download()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssgusGKneiSw"
      },
      "source": [
        "\n",
        "## Spacy\n",
        "![alt text](https://spacy.io/static/social_default-1d3b50b1eba4c2b06244425ff0c49570.jpg)\n",
        "\n",
        "SpaCy is a free and open-source library for Natural Language Processing (NLP) in Python with a lot of in-built capabilities. It’s becoming increasingly popular for processing and analyzing data in NLP. Unstructured textual data is produced at a large scale, and it’s important to process and derive insights from unstructured data. To do that, you need to represent the data in a format that can be understood by computers. NLP can help you do that.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpYLNOO0mynO",
        "outputId": "3402895e-1f6c-402c-9789-0d798f5d2505"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.5.2)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.27.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.9)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.22.4)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.2)\n",
            "2023-05-09 13:13:10.902813: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-09 13:13:13.076997: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-05-09 13:13:15.682270: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-09 13:13:15.682925: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-09 13:13:15.683205: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.5.0) (3.5.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "# to download models and data for the English language\n",
        "!python -m spacy download en_core_web_sm\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8DV3Biwm0dK"
      },
      "source": [
        "\n",
        "## Gensim\n",
        "![alt text](https://repository-images.githubusercontent.com/1349775/202c4680-8f7c-11e9-91c6-745fdcbeffe8)\n",
        "\n",
        "\n",
        "Gensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora. Target audience is the natural language processing (NLP) and information retrieval (IR) community.\n",
        "\n",
        "Features\n",
        "- All algorithms are memory-independent w.r.t. the corpus size (can process input larger than RAM, streamed, out-of-core),\n",
        "- Intuitive interfaces\n",
        "easy to plug in your own input corpus/datastream (simple streaming API)\n",
        "easy to extend with other Vector Space algorithms (simple transformation API)\n",
        "- Efficient multicore implementations of popular algorithms, such as online Latent Semantic Analysis (LSA/LSI/SVD), Latent Dirichlet Allocation (LDA), Random Projections (RP), Hierarchical Dirichlet Process (HDP) or word2vec deep learning.\n",
        "- Distributed computing: can run Latent Semantic Analysis and Latent Dirichlet Allocation on a cluster of computers.\n",
        "\n",
        "The type of input Gensim supports:\n",
        "* As sentences stored in python’s native list object\n",
        "* As one single text file, small or large.\n",
        "* In multiple text files.\n",
        "\n",
        "\n",
        "The core concepts of gensim are:\n",
        "\n",
        "- Document: some text.\n",
        "- Corpus: a collection of documents.\n",
        "- Vector: a mathematically convenient representation of a document.\n",
        "- Model: an algorithm for transforming vectors from one representation to another.\n",
        "\n",
        "Read more in the [documentation](https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.htm)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZL48-qwEwBbq",
        "outputId": "a02c8675-bf7d-484e-f757-b5ce371878b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['human', 'machine', 'interface', 'for', 'lab', 'abc', 'computer', 'applications'], ['a', 'survey', 'of', 'user', 'opinion', 'of', 'computer', 'system', 'response', 'time'], ['the', 'eps', 'user', 'interface', 'management', 'system'], ['system', 'and', 'human', 'system', 'engineering', 'testing', 'of', 'eps'], ['relation', 'of', 'user', 'perceived', 'response', 'time', 'to', 'error', 'measurement'], ['the', 'generation', 'of', 'random', 'binary', 'unordered', 'trees'], ['the', 'intersection', 'graph', 'of', 'paths', 'in', 'trees'], ['graph', 'minors', 'iv', 'widths', 'of', 'trees', 'and', 'well', 'quasi', 'ordering'], ['graph', 'minors', 'a', 'survey']]\n",
            "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]\n"
          ]
        }
      ],
      "source": [
        "from gensim import corpora\n",
        "\n",
        "document = \"Human machine interface for lab abc computer applications\"\n",
        "\n",
        "text_corpus = [\n",
        "    \"Human machine interface for lab abc computer applications\",\n",
        "    \"A survey of user opinion of computer system response time\",\n",
        "    \"The EPS user interface management system\",\n",
        "    \"System and human system engineering testing of EPS\",\n",
        "    \"Relation of user perceived response time to error measurement\",\n",
        "    \"The generation of random binary unordered trees\",\n",
        "    \"The intersection graph of paths in trees\",\n",
        "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
        "    \"Graph minors A survey\",\n",
        "]\n",
        "\n",
        "# Lowercase each document, split it by white space and filter out stopwords\n",
        "texts = [[word for word in document.lower().split()]\n",
        "         for document in text_corpus]\n",
        "\n",
        "print(texts)\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "\n",
        "new_vec = dictionary.doc2bow(document.lower().split())\n",
        "print(new_vec)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3EqehZ6NPJT",
        "outputId": "6fdfb050-ba90-4a60-fe52-782ae72cfbd1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "new_vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rNLxgj1wEi0"
      },
      "source": [
        "\n",
        "\n",
        "## GloVe\n",
        "![alt text](https://miro.medium.com/max/594/0*zjSn1J3eqTXzTxDI.jpg)\n",
        "\n",
        " GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\n",
        "\n",
        "The main intuition underlying the model is the simple observation that ratios of word-word co-occurrence probabilities have the potential for encoding some form of meaning. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zV2Nt8PO0Jjy",
        "outputId": "2545f3d7-021a-498d-d044-d98efd3486a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# WARNING: some of the files are quite large\n",
        "# You can also manually downlaod the pre-trained \n",
        "# wectors and data are here: https://drive.google.com/drive/folders/1Mm3lk1DqIatS08rTVLlO1IxC2drXjCGG?usp=sharing\n",
        "# from Stanford NLP group https://nlp.stanford.edu/projects/glove/.\n",
        "\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Change it with your own path\n",
        "path = '/content/drive/My Drive/Colab Notebooks/DL/input/'\n",
        "comp = 'hate-speech-classification/'\n",
        "EMBEDDING_FILE=f'{path}glove.6B.50d.txt'\n",
        "TRAIN_DATA_FILE=f'{path}{comp}train.csv'\n",
        "TEST_DATA_FILE=f'{path}{comp}test.csv'\n",
        "\n",
        "\n",
        "embed_size = 50 # how big is each word vector\n",
        "max_features = 20000 # how many unique words to use (i.e num rows in embedding vector)\n",
        "maxlen = 100 # max number of words in a comment to use\n",
        "\n",
        "# each line of the file looks like this:\n",
        "# business 0.023693 0.13316 0.023131 ...\n",
        "\n",
        "# Creating a dictionary {word: embedding}\n",
        "embeddings_dict = {}\n",
        "with open(EMBEDDING_FILE, 'r') as f:\n",
        "    lines = [f.readline() for x in range(2)]\n",
        "    for line in lines:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], \"float32\")\n",
        "        embeddings_dict[word] = vector\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3BEOiyA-9DV",
        "outputId": "3821ce1f-388d-42dd-daa9-b99f7f407ad9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the': array([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
              "        -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
              "         2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
              "         1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
              "        -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
              "        -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
              "         4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
              "         7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
              "        -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
              "         1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01],\n",
              "       dtype=float32),\n",
              " ',': array([ 0.013441,  0.23682 , -0.16899 ,  0.40951 ,  0.63812 ,  0.47709 ,\n",
              "        -0.42852 , -0.55641 , -0.364   , -0.23938 ,  0.13001 , -0.063734,\n",
              "        -0.39575 , -0.48162 ,  0.23291 ,  0.090201, -0.13324 ,  0.078639,\n",
              "        -0.41634 , -0.15428 ,  0.10068 ,  0.48891 ,  0.31226 , -0.1252  ,\n",
              "        -0.037512, -1.5179  ,  0.12612 , -0.02442 , -0.042961, -0.28351 ,\n",
              "         3.5416  , -0.11956 , -0.014533, -0.1499  ,  0.21864 , -0.33412 ,\n",
              "        -0.13872 ,  0.31806 ,  0.70358 ,  0.44858 , -0.080262,  0.63003 ,\n",
              "         0.32111 , -0.46765 ,  0.22786 ,  0.36034 , -0.37818 , -0.56657 ,\n",
              "         0.044691,  0.30392 ], dtype=float32)}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "embeddings_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRCf1-yuwZJk"
      },
      "source": [
        "## Huggingface\n",
        "\n",
        "\n",
        "![alt text](https://huggingface.co/favicon.ico)\n",
        "\n",
        "A group that is pushing innovative methods in NLP, in particular transformers, an evolution of word2vect.\n",
        "\n",
        "We will use SimpleTransformer (https://github.com/ThilinaRajapakse/simpletransformers) wich is based on huggingface Transformer.\n",
        "\n",
        "Also provide ultra-fast tokenizatoin using deep learning (Encode 1GB in 20sec)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131,
          "referenced_widgets": [
            "2932607960ea4f1a8a338ab12861703e",
            "ccea2c60e80e442aa07e83d5485bdc56",
            "5d89a66716114c01a9b429c96741f603",
            "f2f007da9cee40d69d160d76b522833a",
            "a846fe3e3cd04da1b99b56ff1773c03d",
            "4c3af867bf1741c49d7ade46ec8a7a0e",
            "47b388c358534bd9843ca7074bf1a717",
            "cadf6bd60ab04b44afe6678dd1cf6eb0",
            "3ef640bc887b4d9d9481d378a3a6a83f",
            "a96fcfccc56d4b5bb05619ad8243e335",
            "d4d1b5fc4f2f4c4b9735bed277cefc6a",
            "abbc12b8bdde487f835efa390912adb1",
            "a629011446a34c90912e62b0a013ce59",
            "45c335466077453aa653a501c8cf60b4",
            "cc57db1bc1af4799b314a79d074efd31",
            "c0a752aa4a774813b0f87997f9435438",
            "54e33040062b4f60a8ee7a4cbc122f71",
            "5345afd8a35a4fd6a25869c8579d743f",
            "4519c9e909024ef392402368c2031ce6",
            "152d1b26614d454b8737a21ff82ca82c",
            "b8bec05a0fef4730a4453eb870cb20c4",
            "478dc353ac6f43128448c85f85f5ab6a",
            "31650d3af55e430a806ea22a78056af3",
            "9d203f2e2b3a4e919d986b5d28e784ba",
            "0d13073921844f8baabc3c04e2b1ba70",
            "7f02b2d45b4b432f82e36c20da00312e",
            "81bfc151ac614dcd934e47d53bed30e7",
            "9ac56932326c47e18f5f7385fa46e4a3",
            "3f596961bede4f04876517fc9e0b28c6",
            "a162b99287b741788ac20c13d445d10a",
            "6579386b7d5a44b7b161c44a78edf5ab",
            "ef9f5a2e04be4f45bd36de37512b16fe",
            "33514b95ca8046f2a65a0101b46c8881"
          ]
        },
        "id": "-NXJNFd0WiP2",
        "outputId": "a19bc681-fd98-4436-8139-12343d5d9a2c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2932607960ea4f1a8a338ab12861703e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "abbc12b8bdde487f835efa390912adb1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "31650d3af55e430a806ea22a78056af3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', \"'\", 'm', 'a', 'new', 'gp', '##u', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer.tokenize(\"I'm a new GPU!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPWwvCSGP0Ad"
      },
      "source": [
        "# Stopwords, stemming, One-Hot Encoding, and more.\n",
        "\n",
        "## What are Stopwords?\n",
        "Stopwords are the most common words in any natural language. For the purpose of analyzing text data and building NLP models, these stopwords might not add much value to the meaning of the document.\n",
        "\n",
        "\n",
        "Generally, the most common words used in a text are “the”, “is”, “in”, “for”, “where”, “when”, “to”, “at” etc.\n",
        "\n",
        "\n",
        "\n",
        "Consider this text string – “There is a pen on the table”. Now, the words “is”, “a”, “on”, and  “the” add no meaning to the statement while parsing it. Whereas words like “there”, “book”, and “table” are the keywords and tell us what the statement is all about\n",
        "\n",
        "\n",
        "## When should we remove stopwords\n",
        "\n",
        "It's usuallly a good idea to remove stopwords in the following tasks:\n",
        "\n",
        "- Text Classification\n",
        "- Spam Filtering\n",
        "- Language Classification\n",
        "- Genre Classification\n",
        "- Caption Generation\n",
        "- Auto-Tag Generation\n",
        " \n",
        "\n",
        "Avoid Stopword Removal\n",
        "- Machine Translation\n",
        "- Language Modeling\n",
        "- Text Summarization\n",
        "- Question-Answering problems\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Stopwords, list vs fewquency\n",
        "\n",
        "In general a mojaor problem in NLP is data sparsness. To combat that a common solution si to eliminate all non-useful information and reduce the ammount of useful information.\n",
        "\n",
        "Advantages in removing Stopwords:\n",
        "\n",
        "- less noise (no info)\n",
        "- less data\n",
        "- better oerfirmances\n",
        "- training time and storage needs decease\n",
        "\n",
        "\n",
        "Disadvantages:: \n",
        "\n",
        "- can completely change the sentence meaning.\n",
        "\n",
        "This pizza is not good can becaome: \"pizza good\" \n",
        "\n",
        "\n",
        "### list\n",
        "\n",
        "There are several known issues in ‘english’ stop word list. It does not aim to be a general, ‘one-size-fits-all’ solution as some tasks may require a more custom solution.\n",
        "\n",
        "\n",
        "Please take care in choosing a stop word list. Popular stop word lists may include words that are highly informative to some tasks, such as computer.\n",
        "\n",
        "\n",
        "You should also make sure that the stop word list has had the same preprocessing and tokenization applied as the one used in the vectorizer. The word we’ve is split into we and ve by CountVectorizer’s default tokenizer, so if we’ve is in stop_words, but ve is not, ve will be retained from we’ve in transformed text. Scikits learn's vectorizers will try to identify and warn about some kinds of inconsistencies\n",
        "\n",
        "Moreover scikits learn only have english stopwords, for completeness we will use NLTK stopwords so it will be easier for you to use it in other languages.\n",
        "\n",
        "### Frequency\n",
        "\n",
        "Another approach is using the fact that stopwords by definition are very common. Therefore it's possible to eliinate them by looking at their frequency.\n",
        "\n",
        "\n",
        "These removal can also create problems. For example how do we establish what's the fequence to identify the stopwords, and how are we going to be sure we will remove alls stopwords?\n",
        "\n",
        "\n",
        "Scikits learn implements several algorithms to count word and token frequency in an efficient way.\n",
        "\n",
        "In Countvectorizer https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html  it's possible to pass the stopwords as an argument. If None, no stop words will be used. max_df can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on intra corpus document frequency of terms.\n",
        "![alt text](https://www.educative.io/api/edpresso/shot/5197621598617600/image/6596233398321152)\n",
        "\n",
        "In practice more we will create an index to increase efficiency.\n",
        "\n",
        "![alt text](https://kavita-ganesan.com/wp-content/uploads/image-5.png)\n",
        "\n",
        "\n",
        "if you wnat to be even more efficient you can use HashingVectorizer. This text vectorizer implementation uses the hashing trick to find the token string name to feature integer index mapping.\n",
        "\n",
        "\n",
        "This strategy has several advantages:\n",
        "\n",
        "- it is very low memory scalable to large datasets as there is no need to store a vocabulary dictionary in memory\n",
        "\n",
        "- it is fast to pickle and un-pickle as it holds no state besides the constructor parameters\n",
        "\n",
        "- it can be used in a streaming (partial fit) or parallel pipeline as there is no state computed during fit.\n",
        "\n",
        "There are also a couple of cons (vs using a CountVectorizer with an in-memory vocabulary):\n",
        "\n",
        "- there is no way to compute the inverse transform (from feature indices to string feature names) which can be a problem when trying to introspect which features are most important to a model.\n",
        "\n",
        "- there can be collisions: distinct tokens can be mapped to the same feature index. However in practice this is rarely an issue if n_features is large enough (e.g. 2 ** 18 for text classification problems).\n",
        "\n",
        "- no IDF weighting as this would render the transformer stateful.\n",
        "\n",
        "\n",
        "\n",
        "## Examples\n",
        "Let's now see some example, we will start by loading the \"20 newsgroups\" dataset in scikits learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "U73kjhDuP0Ae",
        "scrolled": false,
        "outputId": "23c034a6-6cd0-4a56-f18f-d7aaca7d4e16"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'From: jcopelan@nyx.cs.du.edu (The One and Only)\\nSubject: Re: Where are they now?\\nOrganization: Salvation Army Draft Board\\nLines: 31\\n\\nIn article <1ql0d3$5vo@dr-pepper.East.Sun.COM> geoff@East.Sun.COM writes:\\n>Your posting provoked me into checking my save file for memorable\\n>posts. The first I captured was by Ken Arromdee on 19 Feb 1990, on the\\n>subject \"Re: atheist too?\". That was article #473 here; your question\\n>was article #53766, which is an average of about 48 articles a day for\\n>the last three years. As others have noted, the current posting rate is\\n>such that my kill file is depressing large...... Among the posting I\\n>saved in the early days were articles from the following notables:\\n>\\n>>From: loren@sunlight.llnl.gov (Loren Petrich)\\n>>From: jchrist@nazareth.israel.rel (Jesus Christ of Nazareth)\\n>>From: mrc@Tomobiki-Cho.CAC.Washington.EDU (Mark Crispin)\\n>>From: perry@apollo.HP.COM (Jim Perry)\\n>>From: lippard@uavax0.ccit.arizona.edu (James J. Lippard)\\n>>From: minsky@media.mit.edu (Marvin Minsky)\\n>\\n>An interesting bunch.... I wonder where #2 is?\\n\\nDidn\\'t you hear?  His address has changed.  He can be reached at the \\nfollowing address:\\n\\ndkoresh@branch.davidian.compound.waco.tx.us\\n\\nI think he was last seen posting to alt.messianic.\\n\\nJim\\n--\\nIf God is dead and the actor plays his part                    | -- Sting,\\nHis words of fear will find their way to a place in your heart | History\\nWithout the voice of reason every faith is its own curse       | Will Teach Us\\nWithout freedom from the past things can only get worse        | Nothing\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "categories = ['alt.atheism', 'soc.religion.christian',\n",
        "              'comp.graphics', 'sci.med']\n",
        "\n",
        "twenty_train = fetch_20newsgroups(subset='train', \n",
        "                                  categories=categories,\n",
        "                                  shuffle=True,\n",
        "                                  random_state=11)\n",
        "\n",
        "twenty_test = fetch_20newsgroups(subset='test',\n",
        "                                 categories=categories,\n",
        "                                 shuffle=True,\n",
        "                                 random_state=11)\n",
        "\n",
        "\n",
        "twenty_train.keys()\n",
        "\n",
        "sample_text = twenty_train.data[1]\n",
        "sample_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqXJ502Fze9y",
        "outputId": "4761f084-9059-4e9f-fcac-e7c0f4887ff7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['zzz' 'ªl' 'íålittin']\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 4 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "['zyxel v32bis' 'ªl r0506048' 'íålittin no']\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(twenty_train.data)\n",
        "print(vectorizer.get_feature_names_out()[-3:])\n",
        "print(X.toarray()[-3:])\n",
        "\n",
        "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
        "X2 = vectorizer2.fit_transform(twenty_train.data)\n",
        "print(vectorizer2.get_feature_names_out()[-3:])\n",
        "print(X2.toarray()[-3:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRAC4lszP0Ar"
      },
      "source": [
        "## Tokenizer\n",
        "\n",
        "Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens , often removing punctuation. Here is an example of tokenization:\n",
        "\n",
        "Input: \"This is the best movie of all time.\"\n",
        "\n",
        "\n",
        "Output: [\"This\" \"is\" \"the\" \"best\" \"movie\" \"of\" \"all\" \"time\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmyC1UqdP0As",
        "outputId": "882f0b59-aad9-47b6-da71-37f6bc14597a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this', 'is', 'an', ',', 'example']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "sample_text = 'this is an,example'\n",
        "text_tokens = nltk.word_tokenize(sample_text)\n",
        "text_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5VYZphiP0Aw",
        "outputId": "160b9fc8-dc1d-4070-af2d-2569ff5de0de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "# caching stopwords improves performance\n",
        "stopwords_list = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffGYkn_TP0Az"
      },
      "source": [
        "Stopwords removal has to be done before stemming. Since some of stopwords in a text should be stemmed by a stemmer and cannot be filtered by given stopwords anymore. For example, \"was\" turns into \"wa\" by porter stemmer and when you stemmed first before removing stopwords \"wa\" remains in your vector after filtering stopwords which has \"was\" as a stopword. Alternatively, you can run the same stemmer on your set stopwords and then process with stopwords on stemmed text. I'm not sure if there's a tool implicitly runs stemming on given stopwords list to serve there masters, but basically it is not a good idea to put stemming on stopwords because stopwords are not always just a list of tokens from a natural language like english but sometimes they are domain specific words or some others given by practicians and cannot be judged by such toolkits. Therefore, removing stopwords in original form must be done before running stemmer.\n",
        "\n",
        "\n",
        "We need to m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4LA54d9P0A0",
        "outputId": "fa69f5e8-138c-4b4a-e9a5-923dda3abd1a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['No', 'I', 'fly']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "[i for i in ['no', 'No', 'I', 'fly'] if i not in stopwords_list] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kj65Ytq7P0A3",
        "outputId": "58f572d8-34d3-4b3e-a041-cb51387523f0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['fly']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "[i for i in ['no', 'No', 'I', 'fly'] if i.lower() not in stopwords_list] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnKH5U4PP0A7",
        "outputId": "5775f4d0-6ee9-443e-86b1-b22704d77e3b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[',', 'example']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "[i for i in text_tokens if i.lower() not in stopwords_list]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqpzWhckP0A-"
      },
      "source": [
        "## Gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXEpTLiPP0BB",
        "outputId": "00659ecc-468a-4fe2-c920-78a9719dc97f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this', 'is', 'an', 'example']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, strip_tags, strip_punctuation\n",
        "\n",
        "remove_stopwords(\"Better late than never, but better never late.\")\n",
        "# u'Better late never, better late.'\n",
        "\n",
        "# create some\n",
        "\n",
        "CUSTOM_FILTERS = [lambda x: x.lower(), strip_tags, strip_punctuation]\n",
        "\n",
        "processed_string = preprocess_string(sample_text, CUSTOM_FILTERS)\n",
        "processed_string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qts4E9aDP0BE"
      },
      "source": [
        "# Sklearn\n",
        "\n",
        "Stopwords removal can be done inside some algoritms, like CountVectorizer\n",
        "\n",
        "## CountVectorizer\n",
        "\n",
        "Convert a collection of text documents to a matrix of token counts\n",
        "\n",
        "This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix.\n",
        "\n",
        "If you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature selection then the number of features will be equal to the vocabulary size found by analyzing the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OONNDjTMP0BF",
        "outputId": "3136b7ae-62ca-4c44-fdc1-64a6032d8c10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['14' '19' '1990' '1993apr19' '1ql0d3' '21652' '231641' '31' '473' '48'\n",
            " '53766' '5vo' 'about' 'accepted' 'actor' 'address' 'afterwards' 'alt'\n",
            " 'among' 'an' 'and' 'animation' 'anyone' 'anything' 'anywhere' 'apollo'\n",
            " 'are' 'arizona' 'army' 'arromdee' 'article' 'articles' 'as' 'aspect' 'at'\n",
            " 'atheism' 'atheist' 'au' 'average' 'b64635' 'be' 'beaverton' 'board'\n",
            " 'branch' 'bucaille' 'bunch' 'but' 'by' 'cac' 'can' 'captured' 'case'\n",
            " 'caused' 'cc' 'ccit' 'changed' 'checking' 'cho' 'christ' 'colleague'\n",
            " 'com' 'commonly' 'compound' 'cranford' 'crispin' 'cs' 'current' 'curse'\n",
            " 'cwru' 'darice' 'davidian' 'day' 'days' 'dead' 'depressing' 'didn'\n",
            " 'disney' 'dkoresh' 'dr' 'draft' 'du' 'early' 'east' 'edu' 'either' 'even'\n",
            " 'every' 'faith' 'fear' 'feb' 'file' 'find' 'first' 'fly' 'following'\n",
            " 'followup' 'for' 'fred' 'freedom' 'from' 'galacticentric' 'genocide'\n",
            " 'geocentrism' 'geoff' 'get' 'god' 'gov' 'has' 'have' 'he' 'hear' 'heart'\n",
            " 'heliocentric' 'here' 'his' 'history' 'host' 'hp' 'if' 'in' 'inc'\n",
            " 'intention' 'interesting' 'into' 'is' 'islam' 'israel' 'its' 'james'\n",
            " 'jchrist' 'jcopelan' 'jesus' 'jim' 'keith' 'ken' 'kill' 'kmr4' 'labs'\n",
            " 'large' 'last' 'learning' 'line' 'lines' 'lippard' 'llnl' 'long' 'loren'\n",
            " 'mark' 'marvin' 'maurice' 'me' 'media' 'memorable' 'messianic' 'micheal'\n",
            " 'mikec' 'minsky' 'mit' 'monash' 'monu6' 'mrc' 'my' 'nazareth' 'nntp' 'no'\n",
            " 'not' 'notables' 'note' 'noted' 'nothing' 'notion' 'now' 'nyx' 'of' 'on'\n",
            " 'one' 'only' 'or' 'organization' 'others' 'own' 'package' 'part' 'past'\n",
            " 'pepper' 'perry' 'petrich' 'place' 'plays' 'po' 'positive' 'posting'\n",
            " 'posts' 'predictions' 'provoked' 'question' 'qur' 'rate' 're' 'reached'\n",
            " 'reason' 'rel' 'reserve' 'rice' 'ryan' 'sail' 'salvation' 'save' 'saved'\n",
            " 'scientific' 'seen' 'software' 'sole' 'sting' 'student' 'subject' 'such'\n",
            " 'sun' 'sunlight' 'teach' 'tek' 'tektronix' 'tell' 'that' 'their' 'there'\n",
            " 'they' 'things' 'think' 'this' 'three' 'time' 'to' 'tomobiki' 'too' 'tx'\n",
            " 'uavax0' 'university' 'us' 'verse' 'voice' 'waco' 'was' 'washington'\n",
            " 'way' 'were' 'western' 'which' 'while' 'will' 'without' 'wonder' 'words'\n",
            " 'worse' 'writes' 'years' 'you' 'your' 'yoyo']\n",
            "[[0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0 0 1 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0\n",
            "  0 0 0 0 0 0 2 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0\n",
            "  0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1\n",
            "  0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0]\n",
            " [0 1 1 0 1 0 0 1 1 1 1 1 1 0 1 2 0 1 1 2 2 0 0 0 0 1 1 1 1 1 3 2 1 0 1 0\n",
            "  1 0 1 0 1 0 1 1 0 1 0 1 1 2 1 0 0 0 1 1 1 1 1 0 3 0 1 0 1 1 1 1 0 0 1 1\n",
            "  1 1 1 1 0 1 1 1 1 1 2 4 0 0 1 1 1 1 2 1 1 0 2 0 2 0 1 9 0 0 0 1 1 1 1 1\n",
            "  1 2 1 1 0 1 3 1 0 1 1 3 0 0 1 1 6 0 1 1 1 1 1 1 2 0 1 1 0 0 1 2 0 0 1 2\n",
            "  1 0 2 1 1 0 1 1 1 1 0 0 2 1 0 0 1 2 2 0 0 0 1 0 1 1 0 1 1 4 2 1 2 0 1 1\n",
            "  1 0 1 1 1 2 1 1 1 0 0 4 1 0 1 1 0 1 2 1 1 1 0 0 0 0 1 1 1 0 1 0 0 1 0 2\n",
            "  1 2 1 1 0 0 0 2 1 0 1 1 1 0 1 0 2 1 1 1 1 0 2 0 1 1 4 1 1 1 0 1 0 2 2 1\n",
            "  1 1 1 1 1 3 0]\n",
            " [1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 1 2 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1\n",
            "  0 2 0 1 0 0 0 0 1 0 0 2 0 0 0 1 1 2 0 0 0 0 0 0 0 1 0 0 0 0 0 0 2 1 0 0\n",
            "  0 0 0 0 0 0 1 0 0 0 0 4 1 1 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 1 2 0 0 0 0 0\n",
            "  0 0 0 0 1 0 0 0 1 0 0 3 0 1 0 0 4 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0\n",
            "  0 1 0 0 0 1 0 0 0 0 0 0 0 0 2 1 0 1 0 1 2 0 0 0 1 0 3 0 0 3 0 0 0 2 1 0\n",
            "  0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 1 0 2 0 0 0 1 1 1 0 0 0 0 1 0 0 1 0 1 1\n",
            "  0 0 0 0 0 0 0 1 0 2 0 0 0 2 0 2 1 0 0 0 0 1 0 2 0 0 3 0 0 0 1 0 1 0 0 0\n",
            "  0 0 1 0 0 0 1]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords \n",
        "stopwords_list = ['the', 'a', 'where']\n",
        "vectorizer = CountVectorizer(stop_words=stopwords_list)\n",
        "X = vectorizer.fit_transform(twenty_train.data[:3])\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(X.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dE0FlgQP0BR"
      },
      "source": [
        "## Stemming\n",
        "\n",
        "For grammatical reasons, documents are going to use different forms of a word, such as organize, organizes, and organizing. Additionally, there are families of derivationally related words with similar meanings, such as democracy, democratic, and democratization. In many situations, it seems as if it would be useful for a search for one of these words to return documents that contain another word in the set.\n",
        "\n",
        "The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. For instance:\n",
        "\n",
        "am, are, is $\\Rightarrow$ be\n",
        "car, cars, car's, cars' $\\Rightarrow$ car\n",
        "The result of this mapping of text will be something like:\n",
        "the boy's cars are different colors $\\Rightarrow$\n",
        "the boy car be differ color"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bk7WVkJcP0Be",
        "outputId": "ebfa862d-0282-44e3-c306-f62bfc61140f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'appl'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "ps.stem('apples')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rcDRadzDP0Bi",
        "outputId": "80642c51-49a2-440a-a2fd-8ec84d3cdaad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'are'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "ps = SnowballStemmer('english')\n",
        "ps.stem('are')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URJAQ98SP0Bm",
        "outputId": "3f0d8903-c48a-42e5-e502-eceebf585bbf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this', 'is', 'an', 'exampl']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "stemmed_words = [ps.stem(w) for w in processed_string]\n",
        "stemmed_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NiyP8AxP0Bu"
      },
      "source": [
        "## Stemming with Gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6PSHyjuoP0Bu",
        "outputId": "4dc906a3-5840-4db3-eeb6-234d6b64da4f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'appl file'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "from gensim.parsing.porter import PorterStemmer\n",
        "p = PorterStemmer()\n",
        "p.stem_sentence(\"apple files\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3TwN26aTP0Bx",
        "outputId": "6686f516-f17b-4210-e1b2-ea9841b7b915"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'thi is an,exampl'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "p.stem_sentence(sample_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNJZ6G5-P0B0",
        "outputId": "81565222-0dea-48fe-c06d-7847de1072d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['from: mikec@sail.labs.tek.com (micheal cranford) subject: disnei anim organization: tektronix, inc., beaverton, or. lines: 5 ------------------------------------ can anyon tell me anyth about the disnei anim softwar package? note the followup line (thi is not for me but for a colleague).',\n",
              " 'from: jcopelan@nyx.cs.du.edu (the on and only) subject: re: where ar thei now? organization: salvat armi draft board lines: 31 in articl <1ql0d3$5vo@dr-pepper.east.sun.com> geoff@east.sun.com writes: >your post provok me into check my save file for memor >posts. the first i captur wa by ken arromde on 19 feb 1990, on the >subject \"re: atheist too?\". that wa articl #473 here; your question >wa articl #53766, which is an averag of about 48 articl a dai for >the last three years. as other have noted, the current post rate is >such that my kill file is depress large...... among the post i >save in the earli dai were articl from the follow notables: > >>from: loren@sunlight.llnl.gov (loren petrich) >>from: jchrist@nazareth.israel.rel (jesu christ of nazareth) >>from: mrc@tomobiki-cho.cac.washington.edu (mark crispin) >>from: perry@apollo.hp.com (jim perry) >>from: lippard@uavax0.ccit.arizona.edu (jame j. lippard) >>from: minsky@media.mit.edu (marvin minsky) > >an interest bunch.... i wonder where #2 is? didn\\'t you hear? hi address ha changed. he can be reach at the follow address: dkoresh@branch.davidian.compound.waco.tx.u i think he wa last seen post to alt.messianic. jim -- if god is dead and the actor plai hi part | -- sting, hi word of fear will find their wai to a place in your heart | histori without the voic of reason everi faith is it own curs | will teach us without freedom from the past thing can onli get wors | noth',\n",
              " 'from: kmr4@po.cwru.edu (keith m. ryan) subject: re: islam and scientif predict (wa re: genocid is caus by atheism) organization: case western reserv univers lines: 14 nntp-posting-host: b64635.student.cwru.edu in articl <1993apr19.231641.21652@monu6.cc.monash.edu.au> darice@yoyo.cc.monash.edu.au (fred rice) writes: >the posit aspect of thi vers note by dr. mauric bucail is that >while geocentr wa the commonli accept notion at the time (and for >a long time afterwards), there is no notion of geocentr in thi vers >(or anywher in the qur\\'an). there is no notion of heliocentric, or even galacticentr either. -------------------------------------------------------------------------------- \"my sole intent wa learn to fly.\"']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "p.stem_documents(twenty_train.data[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOsy_FliP0B5"
      },
      "source": [
        "## Enhancing Scikits Learn \n",
        "\n",
        "\n",
        "\n",
        "Fancy token-level analysis such as stemming, lemmatizing, compound splitting, filtering based on part-of-speech, etc. are not included in the scikit-learn codebase, but can be added by customizing either the tokenizer or the analyzer. Here’s a CountVectorizer with a tokenizer and lemmatizer using NLTK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eff2E6NP0B5",
        "outputId": "f74d77f7-b264-44af-dd46-2e615f7970f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['#' '$' \"''\" '(' ')' ',' '--' '.' '....' '......' '14' '19' '1990'\n",
            " '1993apr19.231641.21652' '1ql0d3' '2' '31' '473' '48' '5' '53766' '5vo'\n",
            " ':' ';' '<' '>' '?' '@' '``' 'a' 'about' 'accept' 'actor' 'address'\n",
            " 'afterward' 'alt.messian' 'among' 'an' 'and' 'anim' 'anyon' 'anyth'\n",
            " 'anywher' 'apollo.hp.com' 'ar' 'armi' 'arromde' 'articl' 'as' 'aspect'\n",
            " 'at' 'atheism' 'atheist' 'averag' 'b64635.student.cwru.edu' 'be'\n",
            " 'beaverton' 'board' 'branch.davidian.compound.waco.tx.u' 'bucail' 'bunch'\n",
            " 'but' 'by' 'can' 'captur' 'case' 'caus' 'chang' 'check' 'christ'\n",
            " 'colleagu' 'commonli' 'cranford' 'crispin' 'current' 'curs' 'dai' 'daric'\n",
            " 'dead' 'depress' 'did' 'disnei' 'dkoresh' 'dr-pepper.east.sun.com' 'dr.'\n",
            " 'draft' 'earli' 'east.sun.com' 'either' 'even' 'everi' 'faith' 'fear'\n",
            " 'feb' 'file' 'find' 'first' 'fly' 'follow' 'followup' 'for' 'fred'\n",
            " 'freedom' 'from' 'galacticentr' 'genocid' 'geocentr' 'geoff' 'get' 'god'\n",
            " 'ha' 'have' 'he' 'hear' 'heart' 'heliocentr' 'here' 'hi' 'histori' 'i'\n",
            " 'if' 'in' 'inc.' 'intent' 'interest' 'into' 'is' 'islam' 'it' 'j.' 'jame'\n",
            " 'jchrist' 'jcopelan' 'jesu' 'jim' 'keith' 'ken' 'kill' 'kmr4' 'larg'\n",
            " 'last' 'learn' 'line' 'lippard' 'long' 'loren' 'm.' 'mark' 'marvin'\n",
            " 'mauric' 'me' 'media.mit.edu' 'memor' 'micheal' 'mikec' 'minski'\n",
            " 'monu6.cc.monash.edu.au' 'mrc' 'my' \"n't\" 'nazareth'\n",
            " 'nazareth.israel.rel' 'nntp-posting-host' 'no' 'not' 'notabl' 'note'\n",
            " 'noth' 'notion' 'now' 'nyx.cs.du.edu' 'of' 'on' 'onli' 'or' 'organ'\n",
            " 'other' 'own' 'packag' 'part' 'past' 'perri' 'petrich' 'place' 'plai'\n",
            " 'po.cwru.edu' 'posit' 'post' 'predict' 'provok' 'question' \"qur'an\"\n",
            " 'rate' 're' 'reach' 'reason' 'reserv' 'rice' 'ryan' 'sail.labs.tek.com'\n",
            " 'salvat' 'save' 'scientif' 'seen' 'softwar' 'sole' 'sting' 'subject'\n",
            " 'such' 'sunlight.llnl.gov' 'teach' 'tektronix' 'tell' 'that' 'the' 'thei'\n",
            " 'their' 'there' 'thi' 'thing' 'think' 'three' 'time' 'to'\n",
            " 'tomobiki-cho.cac.washington.edu' 'too' 'uavax0.ccit.arizona.edu'\n",
            " 'univers' 'us' 'vers' 'voic' 'wa' 'wai' 'were' 'western' 'where' 'which'\n",
            " 'while' 'will' 'without' 'wonder' 'word' 'wors' 'write' 'year' 'you'\n",
            " 'your' 'yoyo.cc.monash.edu.au' '|']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from gensim.parsing.porter import PorterStemmer\n",
        "from nltk import word_tokenize          \n",
        "from nltk.stem import WordNetLemmatizer \n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "# from nltk.stem import PorterStemmer \n",
        "\n",
        "ps.stem_sentence('apple files')\n",
        "\n",
        "\n",
        "class StemTokenizer:\n",
        "    def __init__(self):\n",
        "        self.ps = PorterStemmer()\n",
        "    def __call__(self, doc):\n",
        "        return [self.ps.stem(t) for t in word_tokenize(doc)]\n",
        "\n",
        "vect = CountVectorizer(tokenizer=StemTokenizer())  \n",
        "\n",
        "\n",
        "X = vect.fit_transform(twenty_train.data[:3])\n",
        "print(vect.get_feature_names_out())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maVlyX_Zk91g"
      },
      "source": [
        "Or even more fancy lemmatizers..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKYKADpbP0B8",
        "outputId": "dd5c3bfe-a8f2-4091-b055-82aa137c991b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['#' '$' \"''\" '(' ')' ',' '--' '.' '....' '......' '14' '19' '1990'\n",
            " '1993apr19.231641.21652' '1ql0d3' '2' '31' '473' '48' '5' '53766' '5vo'\n",
            " ':' ';' '<' '>' '?' '@' '``' 'a' 'about' 'accepted' 'actor' 'address'\n",
            " 'afterwards' 'alt.messianic' 'among' 'an' 'and' 'animation' 'anyone'\n",
            " 'anything' 'anywhere' 'apollo.hp.com' 'are' 'army' 'arromdee' 'article'\n",
            " 'aspect' 'at' 'atheism' 'atheist' 'average' 'b64635.student.cwru.edu'\n",
            " 'be' 'beaverton' 'board' 'branch.davidian.compound.waco.tx.us' 'bucaille'\n",
            " 'bunch' 'but' 'by' 'can' 'captured' 'case' 'caused' 'changed' 'checking'\n",
            " 'christ' 'colleague' 'commonly' 'cranford' 'crispin' 'current' 'curse'\n",
            " 'darice' 'day' 'dead' 'depressing' 'did' 'disney' 'dkoresh'\n",
            " 'dr-pepper.east.sun.com' 'dr.' 'draft' 'early' 'east.sun.com' 'either'\n",
            " 'even' 'every' 'faith' 'fear' 'feb' 'file' 'find' 'first' 'fly'\n",
            " 'following' 'followup' 'for' 'fred' 'freedom' 'from' 'galacticentric'\n",
            " 'genocide' 'geocentrism' 'geoff' 'get' 'god' 'ha' 'have' 'he' 'hear'\n",
            " 'heart' 'heliocentric' 'here' 'his' 'history' 'i' 'if' 'in' 'inc.'\n",
            " 'intention' 'interesting' 'into' 'is' 'islam' 'it' 'j.' 'james' 'jchrist'\n",
            " 'jcopelan' 'jesus' 'jim' 'keith' 'ken' 'kill' 'kmr4' 'large' 'last'\n",
            " 'learning' 'line' 'lippard' 'long' 'loren' 'm.' 'mark' 'marvin' 'maurice'\n",
            " 'me' 'media.mit.edu' 'memorable' 'micheal' 'mikec' 'minsky'\n",
            " 'monu6.cc.monash.edu.au' 'mrc' 'my' \"n't\" 'nazareth'\n",
            " 'nazareth.israel.rel' 'nntp-posting-host' 'no' 'not' 'notable' 'note'\n",
            " 'noted' 'nothing' 'notion' 'now' 'nyx.cs.du.edu' 'of' 'on' 'one' 'only'\n",
            " 'or' 'organization' 'others' 'own' 'package' 'part' 'past' 'perry'\n",
            " 'petrich' 'place' 'play' 'po.cwru.edu' 'positive' 'post' 'posting'\n",
            " 'prediction' 'provoked' 'question' \"qur'an\" 'rate' 're' 'reached'\n",
            " 'reason' 'reserve' 'rice' 'ryan' 'sail.labs.tek.com' 'salvation' 'save'\n",
            " 'saved' 'scientific' 'seen' 'software' 'sole' 'sting' 'subject' 'such'\n",
            " 'sunlight.llnl.gov' 'teach' 'tektronix' 'tell' 'that' 'the' 'their'\n",
            " 'there' 'they' 'thing' 'think' 'this' 'three' 'time' 'to'\n",
            " 'tomobiki-cho.cac.washington.edu' 'too' 'u' 'uavax0.ccit.arizona.edu'\n",
            " 'university' 'verse' 'voice' 'wa' 'way' 'were' 'western' 'where' 'which'\n",
            " 'while' 'will' 'without' 'wonder' 'word' 'worse' 'writes' 'year' 'you'\n",
            " 'your' 'yoyo.cc.monash.edu.au' '|']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "nltk.download('wordnet')\n",
        "\n",
        "class LemmaTokenizer:\n",
        "    def __init__(self):\n",
        "        self.wnl = WordNetLemmatizer()\n",
        "    def __call__(self, doc):\n",
        "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
        "\n",
        "vect = CountVectorizer(tokenizer=LemmaTokenizer())  \n",
        "\n",
        "X = vect.fit_transform(twenty_train.data[:3])\n",
        "print(vect.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CndH0e-_lXmK"
      },
      "source": [
        "you can now compare it with the simple impementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aolpK3_C29Z"
      },
      "source": [
        "## Modern problems\n",
        "\n",
        "Do you enjoy emojis? Well let's see after you find them in your dataset. Lukcly there are libraries that help with this problem.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aC4C9eaYDUHQ"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacymoji import Emoji\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "emoji = Emoji(nlp)\n",
        "nlp.add_pipe(\"emoji\", first=True)\n",
        "\n",
        "doc = nlp(u\"This is a test 😻 👍🏿\")\n",
        "assert doc._.has_emoji == True\n",
        "assert doc[2:5]._.has_emoji == True\n",
        "assert doc[0]._.is_emoji == False\n",
        "assert doc[4]._.is_emoji == True\n",
        "assert doc[5]._.emoji_desc == u'thumbs up dark skin tone'\n",
        "assert len(doc._.emoji) == 2\n",
        "assert doc._.emoji[1] == (u'👍🏿', 5, u'thumbs up dark skin tone')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzPhhInCIteS"
      },
      "source": [
        "It's also possible to fix spelling mistakes "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OMC1jRPJpn1",
        "outputId": "61acdfff-0cb0-4bf5-fb9d-7c136a8dd55c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word\n"
          ]
        }
      ],
      "source": [
        "from spellchecker import SpellChecker\n",
        "\n",
        "spell = SpellChecker()\n",
        "\n",
        "print(spell.correction('kword'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUbg8F9tP0CF"
      },
      "source": [
        "## Part-Of-Speech tagging\n",
        "\n",
        "\n",
        "Somethign we missed so far are information about the role of the words in the sentence. Many words can be used as verb and noun depending on the context. \"Help\" for example cna be used as a verb: (I help you) or noun (thanks for your help) and if it was stemmed it couls also have been an adjective (helpfull).\n",
        "\n",
        "Part-Of-Speech (POS) tagging consist on adding some extra information to the word to specify its role in the sentence. \n",
        "\n",
        "For Example\n",
        "\n",
        "I (PRP) eat (VB) pizza (NN)\n",
        "\n",
        "Where PRP stands for personal pronoun, VB for verb and NN for noun.\n",
        "\n",
        "Methods: \n",
        "\n",
        "\n",
        "- Lexical Based Methods — Assigns the POS tag the most frequently occurring with a word in the training corpus.\n",
        "\n",
        "- Rule-Based Methods — Assigns POS tags based on rules. For example, we can have a rule that says, words ending with “ed” or “ing” must be assigned to a verb. Rule-Based Techniques can be used along with Lexical Based approaches to allow POS Tagging of words that are not present in the training corpus but are there in the testing data.\n",
        "\n",
        "- Probabilistic Methods — This method assigns the POS tags based on the probability of a particular tag sequence occurring.\n",
        "We can distinguish further in \n",
        "discriminative methods who try to model the conditional probability distribution, i.e., P(y|x) like SVM, logistic regression and Conditional Random Fields (CRFs)\n",
        "and generative methods, like naive baise and Hidden Markov Models (HMMs) are probabilistic approaches to assign a POS Tag.\n",
        "\n",
        "- Deep Learning Methods — Recurrent Neural Networks can also be used for POS tagging.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPJKaMojP0CI"
      },
      "source": [
        "#### Perceptron tagger in the NLTK package \n",
        "\n",
        "We will now see the NLTK implementation of CRF trained using the treebank dataset. Do do that we need to downlaoad the dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVk9lAT4P0CK",
        "outputId": "fc5931de-e77e-4a88-c157-d6af0601a789"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udkUPRPuP0CS",
        "outputId": "b7de9f16-c2b9-4530-baa5-53f9e4e7e2a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "nltk.download('treebank')\n",
        "nltk.download('universal_tagset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2n74Uh3P0CW",
        "outputId": "639a51c2-dc21-4410-b8f0-34ce02426629"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('This', 'DT'), ('is', 'VBZ'), ('an', 'DT'), ('example', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# The default pos tagger model using in NLTK is maxent_treebanck_pos_tagger\n",
        "# model, you can find the code in nltk-master/nltk/tag/__init__.py:\n",
        "\n",
        "nltk.pos_tag('This is an example'.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2GjAQX9P0Ca",
        "scrolled": false,
        "outputId": "75bb5a80-8b41-4442-85f1-e3a517192c16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Tagged Sentences  3914\n",
            "Total Number of Tagged words 100676\n",
            "Vocabulary of the Corpus 12408\n",
            "Number of Tags in the Corpus  12\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "tagged_sentence = nltk.corpus.treebank.tagged_sents(tagset='universal')\n",
        "print(\"Number of Tagged Sentences \",len(tagged_sentence))\n",
        "tagged_words=[tup for sent in tagged_sentence for tup in sent]\n",
        "print(\"Total Number of Tagged words\", len(tagged_words))\n",
        "vocab=set([word for word,tag in tagged_words])\n",
        "print(\"Vocabulary of the Corpus\",len(vocab))\n",
        "tags=set([tag for word,tag in tagged_words])\n",
        "print(\"Number of Tags in the Corpus \",len(tags))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZDBTBcdP0Ce"
      },
      "source": [
        "## Lemmatization\n",
        "\n",
        "Lemmatization goes a step forward than stemming. It involves the understanding of the role of the words in the sentence.\n",
        "\n",
        "It relyis generally on Part-Of-Speech (POS) tagging.\n",
        "POS Tags are useful for building parse trees, which are used in building Name Entity Recogmition's (most named entities are Nouns) and extracting relations between words. It is also essential for building lemmatizers which are used to reduce a word to its root form.\n",
        "\n",
        "For example meeting/NOUN should be \"stemmed\" to meeting, not \"meet\". On the other hand meeting/VERB should be stemmed \"meet\".\n",
        "\n",
        "\n",
        "Stememr therefor is better for very specialized domains (medical) or languages that dont have a proper POS dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "APq3bhEtP0Cf",
        "outputId": "30d4fcbd-8ee7-4480-a3eb-6e3ef671fba3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-94db9d92c987>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m lookups.add_table({\n\u001b[0m\u001b[1;32m     15\u001b[0m     \"lemma_rules\", {\n\u001b[1;32m     16\u001b[0m         \u001b[0;34m\"noun\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'dict'"
          ]
        }
      ],
      "source": [
        "# https://spacy.io/api/lemmatizer#lookup\n",
        "# from spacy.lemmatizer import Lemmatizer\n",
        "import spacy\n",
        "from spacy.lookups import Lookups\n",
        "# This class allows convenient access to large lookup tables and dictionaries, e.g. lemmatization data or \n",
        "# tokenizer exception lists using Bloom filters.\n",
        "lookups = Lookups()\n",
        "# Add a new table with optional data to the lookups.\n",
        "# Raises an error if the table exists.\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "# lemmatizer = nlp.add_pipe(\"lemmatizer\")\n",
        "\n",
        "\n",
        "lookups.add_table({\n",
        "    \"lemma_rules\", {\n",
        "        \"noun\": [\"s\", \"\"], \n",
        "        \"verb\": [\"ing\", \"\"]}\n",
        "                             })\n",
        "# lemmatizer = lemmatizer.lookup_lemmatize(lookups)\n",
        "# lemmatizer('books', 'noun')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjTY7DxgP0Cj"
      },
      "source": [
        "# Category encoders\n",
        "\n",
        "A major problem with text is that it does not have all the woderfull properties that numbers have.\n",
        "\n",
        "\n",
        "There are many methods to transform vategorical data into numeric values, we saw one beofre: CountVectorizer\n",
        "\n",
        "Convert a collection of text documents to a matrix of token counts\n",
        "\n",
        "This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix.\n",
        "\n",
        "\n",
        "It's possible to specify several parameters for the CountVectorizer, for example if you want to analyze token or words, and if you want to include n_grams which inicates how many consecutive tokens we want to consider."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_meL5abELFDa",
        "outputId": "266cdc4e-6d47-4a3a-f539-e1cdc1e2c38c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class CountVectorizer in module sklearn.feature_extraction.text:\n",
            "\n",
            "class CountVectorizer(_VectorizerMixin, sklearn.base.BaseEstimator)\n",
            " |  CountVectorizer(*, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)\n",
            " |  \n",
            " |  Convert a collection of text documents to a matrix of token counts.\n",
            " |  \n",
            " |  This implementation produces a sparse representation of the counts using\n",
            " |  scipy.sparse.csr_matrix.\n",
            " |  \n",
            " |  If you do not provide an a-priori dictionary and you do not use an analyzer\n",
            " |  that does some kind of feature selection then the number of features will\n",
            " |  be equal to the vocabulary size found by analyzing the data.\n",
            " |  \n",
            " |  Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
            " |  \n",
            " |  Parameters\n",
            " |  ----------\n",
            " |  input : {'filename', 'file', 'content'}, default='content'\n",
            " |      - If `'filename'`, the sequence passed as an argument to fit is\n",
            " |        expected to be a list of filenames that need reading to fetch\n",
            " |        the raw content to analyze.\n",
            " |  \n",
            " |      - If `'file'`, the sequence items must have a 'read' method (file-like\n",
            " |        object) that is called to fetch the bytes in memory.\n",
            " |  \n",
            " |      - If `'content'`, the input is expected to be a sequence of items that\n",
            " |        can be of type string or byte.\n",
            " |  \n",
            " |  encoding : str, default='utf-8'\n",
            " |      If bytes or files are given to analyze, this encoding is used to\n",
            " |      decode.\n",
            " |  \n",
            " |  decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
            " |      Instruction on what to do if a byte sequence is given to analyze that\n",
            " |      contains characters not of the given `encoding`. By default, it is\n",
            " |      'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
            " |      values are 'ignore' and 'replace'.\n",
            " |  \n",
            " |  strip_accents : {'ascii', 'unicode'} or callable, default=None\n",
            " |      Remove accents and perform other character normalization\n",
            " |      during the preprocessing step.\n",
            " |      'ascii' is a fast method that only works on characters that have\n",
            " |      a direct ASCII mapping.\n",
            " |      'unicode' is a slightly slower method that works on any characters.\n",
            " |      None (default) does nothing.\n",
            " |  \n",
            " |      Both 'ascii' and 'unicode' use NFKD normalization from\n",
            " |      :func:`unicodedata.normalize`.\n",
            " |  \n",
            " |  lowercase : bool, default=True\n",
            " |      Convert all characters to lowercase before tokenizing.\n",
            " |  \n",
            " |  preprocessor : callable, default=None\n",
            " |      Override the preprocessing (strip_accents and lowercase) stage while\n",
            " |      preserving the tokenizing and n-grams generation steps.\n",
            " |      Only applies if ``analyzer`` is not callable.\n",
            " |  \n",
            " |  tokenizer : callable, default=None\n",
            " |      Override the string tokenization step while preserving the\n",
            " |      preprocessing and n-grams generation steps.\n",
            " |      Only applies if ``analyzer == 'word'``.\n",
            " |  \n",
            " |  stop_words : {'english'}, list, default=None\n",
            " |      If 'english', a built-in stop word list for English is used.\n",
            " |      There are several known issues with 'english' and you should\n",
            " |      consider an alternative (see :ref:`stop_words`).\n",
            " |  \n",
            " |      If a list, that list is assumed to contain stop words, all of which\n",
            " |      will be removed from the resulting tokens.\n",
            " |      Only applies if ``analyzer == 'word'``.\n",
            " |  \n",
            " |      If None, no stop words will be used. In this case, setting `max_df`\n",
            " |      to a higher value, such as in the range (0.7, 1.0), can automatically detect\n",
            " |      and filter stop words based on intra corpus document frequency of terms.\n",
            " |  \n",
            " |  token_pattern : str or None, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n",
            " |      Regular expression denoting what constitutes a \"token\", only used\n",
            " |      if ``analyzer == 'word'``. The default regexp select tokens of 2\n",
            " |      or more alphanumeric characters (punctuation is completely ignored\n",
            " |      and always treated as a token separator).\n",
            " |  \n",
            " |      If there is a capturing group in token_pattern then the\n",
            " |      captured group content, not the entire match, becomes the token.\n",
            " |      At most one capturing group is permitted.\n",
            " |  \n",
            " |  ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
            " |      The lower and upper boundary of the range of n-values for different\n",
            " |      word n-grams or char n-grams to be extracted. All values of n such\n",
            " |      such that min_n <= n <= max_n will be used. For example an\n",
            " |      ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n",
            " |      unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n",
            " |      Only applies if ``analyzer`` is not callable.\n",
            " |  \n",
            " |  analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n",
            " |      Whether the feature should be made of word n-gram or character\n",
            " |      n-grams.\n",
            " |      Option 'char_wb' creates character n-grams only from text inside\n",
            " |      word boundaries; n-grams at the edges of words are padded with space.\n",
            " |  \n",
            " |      If a callable is passed it is used to extract the sequence of features\n",
            " |      out of the raw, unprocessed input.\n",
            " |  \n",
            " |      .. versionchanged:: 0.21\n",
            " |  \n",
            " |      Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
            " |      first read from the file and then passed to the given callable\n",
            " |      analyzer.\n",
            " |  \n",
            " |  max_df : float in range [0.0, 1.0] or int, default=1.0\n",
            " |      When building the vocabulary ignore terms that have a document\n",
            " |      frequency strictly higher than the given threshold (corpus-specific\n",
            " |      stop words).\n",
            " |      If float, the parameter represents a proportion of documents, integer\n",
            " |      absolute counts.\n",
            " |      This parameter is ignored if vocabulary is not None.\n",
            " |  \n",
            " |  min_df : float in range [0.0, 1.0] or int, default=1\n",
            " |      When building the vocabulary ignore terms that have a document\n",
            " |      frequency strictly lower than the given threshold. This value is also\n",
            " |      called cut-off in the literature.\n",
            " |      If float, the parameter represents a proportion of documents, integer\n",
            " |      absolute counts.\n",
            " |      This parameter is ignored if vocabulary is not None.\n",
            " |  \n",
            " |  max_features : int, default=None\n",
            " |      If not None, build a vocabulary that only consider the top\n",
            " |      `max_features` ordered by term frequency across the corpus.\n",
            " |      Otherwise, all features are used.\n",
            " |  \n",
            " |      This parameter is ignored if vocabulary is not None.\n",
            " |  \n",
            " |  vocabulary : Mapping or iterable, default=None\n",
            " |      Either a Mapping (e.g., a dict) where keys are terms and values are\n",
            " |      indices in the feature matrix, or an iterable over terms. If not\n",
            " |      given, a vocabulary is determined from the input documents. Indices\n",
            " |      in the mapping should not be repeated and should not have any gap\n",
            " |      between 0 and the largest index.\n",
            " |  \n",
            " |  binary : bool, default=False\n",
            " |      If True, all non zero counts are set to 1. This is useful for discrete\n",
            " |      probabilistic models that model binary events rather than integer\n",
            " |      counts.\n",
            " |  \n",
            " |  dtype : dtype, default=np.int64\n",
            " |      Type of the matrix returned by fit_transform() or transform().\n",
            " |  \n",
            " |  Attributes\n",
            " |  ----------\n",
            " |  vocabulary_ : dict\n",
            " |      A mapping of terms to feature indices.\n",
            " |  \n",
            " |  fixed_vocabulary_ : bool\n",
            " |      True if a fixed vocabulary of term to indices mapping\n",
            " |      is provided by the user.\n",
            " |  \n",
            " |  stop_words_ : set\n",
            " |      Terms that were ignored because they either:\n",
            " |  \n",
            " |        - occurred in too many documents (`max_df`)\n",
            " |        - occurred in too few documents (`min_df`)\n",
            " |        - were cut off by feature selection (`max_features`).\n",
            " |  \n",
            " |      This is only available if no vocabulary was given.\n",
            " |  \n",
            " |  See Also\n",
            " |  --------\n",
            " |  HashingVectorizer : Convert a collection of text documents to a\n",
            " |      matrix of token counts.\n",
            " |  \n",
            " |  TfidfVectorizer : Convert a collection of raw documents to a matrix\n",
            " |      of TF-IDF features.\n",
            " |  \n",
            " |  Notes\n",
            " |  -----\n",
            " |  The ``stop_words_`` attribute can get large and increase the model size\n",
            " |  when pickling. This attribute is provided only for introspection and can\n",
            " |  be safely removed using delattr or set to None before pickling.\n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> from sklearn.feature_extraction.text import CountVectorizer\n",
            " |  >>> corpus = [\n",
            " |  ...     'This is the first document.',\n",
            " |  ...     'This document is the second document.',\n",
            " |  ...     'And this is the third one.',\n",
            " |  ...     'Is this the first document?',\n",
            " |  ... ]\n",
            " |  >>> vectorizer = CountVectorizer()\n",
            " |  >>> X = vectorizer.fit_transform(corpus)\n",
            " |  >>> vectorizer.get_feature_names_out()\n",
            " |  array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
            " |         'this'], ...)\n",
            " |  >>> print(X.toarray())\n",
            " |  [[0 1 1 1 0 0 1 0 1]\n",
            " |   [0 2 0 1 0 1 1 0 1]\n",
            " |   [1 0 0 1 1 0 1 1 1]\n",
            " |   [0 1 1 1 0 0 1 0 1]]\n",
            " |  >>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
            " |  >>> X2 = vectorizer2.fit_transform(corpus)\n",
            " |  >>> vectorizer2.get_feature_names_out()\n",
            " |  array(['and this', 'document is', 'first document', 'is the', 'is this',\n",
            " |         'second document', 'the first', 'the second', 'the third', 'third one',\n",
            " |         'this document', 'this is', 'this the'], ...)\n",
            " |   >>> print(X2.toarray())\n",
            " |   [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
            " |   [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
            " |   [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
            " |   [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      CountVectorizer\n",
            " |      _VectorizerMixin\n",
            " |      sklearn.base.BaseEstimator\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, *, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  fit(self, raw_documents, y=None)\n",
            " |      Learn a vocabulary dictionary of all tokens in the raw documents.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      raw_documents : iterable\n",
            " |          An iterable which generates either str, unicode or file objects.\n",
            " |      \n",
            " |      y : None\n",
            " |          This parameter is ignored.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |          Fitted vectorizer.\n",
            " |  \n",
            " |  fit_transform(self, raw_documents, y=None)\n",
            " |      Learn the vocabulary dictionary and return document-term matrix.\n",
            " |      \n",
            " |      This is equivalent to fit followed by transform, but more efficiently\n",
            " |      implemented.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      raw_documents : iterable\n",
            " |          An iterable which generates either str, unicode or file objects.\n",
            " |      \n",
            " |      y : None\n",
            " |          This parameter is ignored.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      X : array of shape (n_samples, n_features)\n",
            " |          Document-term matrix.\n",
            " |  \n",
            " |  get_feature_names_out(self, input_features=None)\n",
            " |      Get output feature names for transformation.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      input_features : array-like of str or None, default=None\n",
            " |          Not used, present here for API consistency by convention.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      feature_names_out : ndarray of str objects\n",
            " |          Transformed feature names.\n",
            " |  \n",
            " |  inverse_transform(self, X)\n",
            " |      Return terms per document with nonzero entries in X.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          Document-term matrix.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      X_inv : list of arrays of shape (n_samples,)\n",
            " |          List of arrays of terms.\n",
            " |  \n",
            " |  transform(self, raw_documents)\n",
            " |      Transform documents to document-term matrix.\n",
            " |      \n",
            " |      Extract token counts out of raw text documents using the vocabulary\n",
            " |      fitted with fit or the one provided to the constructor.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      raw_documents : iterable\n",
            " |          An iterable which generates either str, unicode or file objects.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      X : sparse matrix of shape (n_samples, n_features)\n",
            " |          Document-term matrix.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from _VectorizerMixin:\n",
            " |  \n",
            " |  build_analyzer(self)\n",
            " |      Return a callable to process input data.\n",
            " |      \n",
            " |      The callable handles preprocessing, tokenization, and n-grams generation.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      analyzer: callable\n",
            " |          A function to handle preprocessing, tokenization\n",
            " |          and n-grams generation.\n",
            " |  \n",
            " |  build_preprocessor(self)\n",
            " |      Return a function to preprocess the text before tokenization.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      preprocessor: callable\n",
            " |            A function to preprocess the text before tokenization.\n",
            " |  \n",
            " |  build_tokenizer(self)\n",
            " |      Return a function that splits a string into a sequence of tokens.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      tokenizer: callable\n",
            " |            A function to split a string into a sequence of tokens.\n",
            " |  \n",
            " |  decode(self, doc)\n",
            " |      Decode the input into a string of unicode symbols.\n",
            " |      \n",
            " |      The decoding strategy depends on the vectorizer parameters.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      doc : bytes or str\n",
            " |          The string to decode.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      doc: str\n",
            " |          A string of unicode symbols.\n",
            " |  \n",
            " |  get_stop_words(self)\n",
            " |      Build or fetch the effective stop words list.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      stop_words: list or None\n",
            " |              A list of stop words.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from _VectorizerMixin:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.BaseEstimator:\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __repr__(self, N_CHAR_MAX=700)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  get_params(self, deep=True)\n",
            " |      Get parameters for this estimator.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      deep : bool, default=True\n",
            " |          If True, will return the parameters for this estimator and\n",
            " |          contained subobjects that are estimators.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      params : dict\n",
            " |          Parameter names mapped to their values.\n",
            " |  \n",
            " |  set_params(self, **params)\n",
            " |      Set the parameters of this estimator.\n",
            " |      \n",
            " |      The method works on simple estimators as well as on nested objects\n",
            " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
            " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
            " |      possible to update each component of a nested object.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      **params : dict\n",
            " |          Estimator parameters.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : estimator instance\n",
            " |          Estimator instance.\n",
            "\n",
            "['14' '19' '1990' '1993apr19' '1ql0d3' '21652' '231641' '31' '473' '48'\n",
            " '53766' '5vo' 'about' 'accepted' 'actor' 'address' 'afterwards' 'alt'\n",
            " 'among' 'an' 'and' 'animation' 'anyone' 'anything' 'anywhere' 'apollo'\n",
            " 'are' 'arizona' 'army' 'arromdee' 'article' 'articles' 'as' 'aspect' 'at'\n",
            " 'atheism' 'atheist' 'au' 'average' 'b64635' 'be' 'beaverton' 'board'\n",
            " 'branch' 'bucaille' 'bunch' 'but' 'by' 'cac' 'can' 'captured' 'case'\n",
            " 'caused' 'cc' 'ccit' 'changed' 'checking' 'cho' 'christ' 'colleague'\n",
            " 'com' 'commonly' 'compound' 'cranford' 'crispin' 'cs' 'current' 'curse'\n",
            " 'cwru' 'darice' 'davidian' 'day' 'days' 'dead' 'depressing' 'didn'\n",
            " 'disney' 'dkoresh' 'dr' 'draft' 'du' 'early' 'east' 'edu' 'either' 'even'\n",
            " 'every' 'faith' 'fear' 'feb' 'file' 'find' 'first' 'fly' 'following'\n",
            " 'followup' 'for' 'fred' 'freedom' 'from' 'galacticentric' 'genocide'\n",
            " 'geocentrism' 'geoff' 'get' 'god' 'gov' 'has' 'have' 'he' 'hear' 'heart'\n",
            " 'heliocentric' 'here' 'his' 'history' 'host' 'hp' 'if' 'in' 'inc'\n",
            " 'intention' 'interesting' 'into' 'is' 'islam' 'israel' 'its' 'james'\n",
            " 'jchrist' 'jcopelan' 'jesus' 'jim' 'keith' 'ken' 'kill' 'kmr4' 'labs'\n",
            " 'large' 'last' 'learning' 'line' 'lines' 'lippard' 'llnl' 'long' 'loren'\n",
            " 'mark' 'marvin' 'maurice' 'me' 'media' 'memorable' 'messianic' 'micheal'\n",
            " 'mikec' 'minsky' 'mit' 'monash' 'monu6' 'mrc' 'my' 'nazareth' 'nntp' 'no'\n",
            " 'not' 'notables' 'note' 'noted' 'nothing' 'notion' 'now' 'nyx' 'of' 'on'\n",
            " 'one' 'only' 'or' 'organization' 'others' 'own' 'package' 'part' 'past'\n",
            " 'pepper' 'perry' 'petrich' 'place' 'plays' 'po' 'positive' 'posting'\n",
            " 'posts' 'predictions' 'provoked' 'question' 'qur' 'rate' 're' 'reached'\n",
            " 'reason' 'rel' 'reserve' 'rice' 'ryan' 'sail' 'salvation' 'save' 'saved'\n",
            " 'scientific' 'seen' 'software' 'sole' 'sting' 'student' 'subject' 'such'\n",
            " 'sun' 'sunlight' 'teach' 'tek' 'tektronix' 'tell' 'that' 'the' 'their'\n",
            " 'there' 'they' 'things' 'think' 'this' 'three' 'time' 'to' 'tomobiki'\n",
            " 'too' 'tx' 'uavax0' 'university' 'us' 'verse' 'voice' 'waco' 'was'\n",
            " 'washington' 'way' 'were' 'western' 'where' 'which' 'while' 'will'\n",
            " 'without' 'wonder' 'words' 'worse' 'writes' 'years' 'you' 'your' 'yoyo']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "help(CountVectorizer)\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(twenty_train.data[:3])\n",
        "print(vectorizer.get_feature_names_out())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAZkveMlP0Cj",
        "outputId": "d40e21e7-698a-47c0-ce3e-87858a0cf8f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['14' '14 nntp' '19' '19 feb' '1990' '1990 on' '1993apr19'\n",
            " '1993apr19 231641' '1ql0d3' '1ql0d3 5vo' '21652' '21652 monu6' '231641'\n",
            " '231641 21652' '31' '31 in' '473' '473 here' '48' '48 articles' '53766'\n",
            " '53766 which' '5vo' '5vo dr' 'about' 'about 48' 'about the' 'accepted'\n",
            " 'accepted notion' 'actor' 'actor plays' 'address' 'address dkoresh'\n",
            " 'address has' 'afterwards' 'afterwards there' 'alt' 'alt messianic'\n",
            " 'among' 'among the' 'an' 'an average' 'an interesting' 'an there' 'and'\n",
            " 'and for' 'and only' 'and scientific' 'and the' 'animation'\n",
            " 'animation organization' 'animation software' 'anyone' 'anyone tell'\n",
            " 'anything' 'anything about' 'anywhere' 'anywhere in' 'apollo' 'apollo hp'\n",
            " 'are' 'are they' 'arizona' 'arizona edu' 'army' 'army draft' 'arromdee'\n",
            " 'arromdee on' 'article' 'article 1993apr19' 'article 1ql0d3'\n",
            " 'article 473' 'article 53766' 'articles' 'articles day' 'articles from'\n",
            " 'as' 'as others' 'aspect' 'aspect of' 'at' 'at the' 'atheism'\n",
            " 'atheism organization' 'atheist' 'atheist too' 'au' 'au darice' 'au fred'\n",
            " 'average' 'average of' 'b64635' 'b64635 student' 'be' 'be reached'\n",
            " 'beaverton' 'beaverton or' 'board' 'board lines' 'branch'\n",
            " 'branch davidian' 'bucaille' 'bucaille is' 'bunch' 'bunch wonder' 'but'\n",
            " 'but for' 'by' 'by atheism' 'by dr' 'by ken' 'cac' 'cac washington' 'can'\n",
            " 'can anyone' 'can be' 'can only' 'captured' 'captured was' 'case'\n",
            " 'case western' 'caused' 'caused by' 'cc' 'cc monash' 'ccit'\n",
            " 'ccit arizona' 'changed' 'changed he' 'checking' 'checking my' 'cho'\n",
            " 'cho cac' 'christ' 'christ of' 'colleague' 'com' 'com geoff' 'com jim'\n",
            " 'com micheal' 'com writes' 'commonly' 'commonly accepted' 'compound'\n",
            " 'compound waco' 'cranford' 'cranford subject' 'crispin' 'crispin from'\n",
            " 'cs' 'cs du' 'current' 'current posting' 'curse' 'curse will' 'cwru'\n",
            " 'cwru edu' 'darice' 'darice yoyo' 'davidian' 'davidian compound' 'day'\n",
            " 'day for' 'days' 'days were' 'dead' 'dead and' 'depressing'\n",
            " 'depressing large' 'didn' 'didn you' 'disney' 'disney animation'\n",
            " 'dkoresh' 'dkoresh branch' 'dr' 'dr maurice' 'dr pepper' 'draft'\n",
            " 'draft board' 'du' 'du edu' 'early' 'early days' 'east' 'east sun' 'edu'\n",
            " 'edu au' 'edu in' 'edu james' 'edu keith' 'edu mark' 'edu marvin'\n",
            " 'edu the' 'either' 'either my' 'even' 'even galacticentric' 'every'\n",
            " 'every faith' 'faith' 'faith is' 'fear' 'fear will' 'feb' 'feb 1990'\n",
            " 'file' 'file for' 'file is' 'find' 'find their' 'first' 'first captured'\n",
            " 'fly' 'following' 'following address' 'following notables' 'followup'\n",
            " 'followup line' 'for' 'for colleague' 'for long' 'for me' 'for memorable'\n",
            " 'for the' 'fred' 'fred rice' 'freedom' 'freedom from' 'from'\n",
            " 'from jchrist' 'from jcopelan' 'from kmr4' 'from lippard' 'from loren'\n",
            " 'from mikec' 'from minsky' 'from mrc' 'from perry' 'from the'\n",
            " 'galacticentric' 'galacticentric either' 'genocide' 'genocide is'\n",
            " 'geocentrism' 'geocentrism in' 'geocentrism was' 'geoff' 'geoff east'\n",
            " 'get' 'get worse' 'god' 'god is' 'gov' 'gov loren' 'has' 'has changed'\n",
            " 'have' 'have noted' 'he' 'he can' 'he was' 'hear' 'hear his' 'heart'\n",
            " 'heart history' 'heliocentric' 'heliocentric or' 'here' 'here your' 'his'\n",
            " 'his address' 'his part' 'his words' 'history' 'history without' 'host'\n",
            " 'host b64635' 'hp' 'hp com' 'if' 'if god' 'in' 'in article' 'in the'\n",
            " 'in this' 'in your' 'inc' 'inc beaverton' 'intention' 'intention was'\n",
            " 'interesting' 'interesting bunch' 'into' 'into checking' 'is' 'is an'\n",
            " 'is caused' 'is dead' 'is depressing' 'is didn' 'is its' 'is no' 'is not'\n",
            " 'is such' 'is that' 'islam' 'islam and' 'israel' 'israel rel' 'its'\n",
            " 'its own' 'james' 'james lippard' 'jchrist' 'jchrist nazareth' 'jcopelan'\n",
            " 'jcopelan nyx' 'jesus' 'jesus christ' 'jim' 'jim if' 'jim perry' 'keith'\n",
            " 'keith ryan' 'ken' 'ken arromdee' 'kill' 'kill file' 'kmr4' 'kmr4 po'\n",
            " 'labs' 'labs tek' 'large' 'large among' 'last' 'last seen' 'last three'\n",
            " 'learning' 'learning to' 'line' 'line this' 'lines' 'lines 14' 'lines 31'\n",
            " 'lines can' 'lippard' 'lippard from' 'lippard uavax0' 'llnl' 'llnl gov'\n",
            " 'long' 'long time' 'loren' 'loren petrich' 'loren sunlight' 'mark'\n",
            " 'mark crispin' 'marvin' 'marvin minsky' 'maurice' 'maurice bucaille' 'me'\n",
            " 'me anything' 'me but' 'me into' 'media' 'media mit' 'memorable'\n",
            " 'memorable posts' 'messianic' 'messianic jim' 'micheal'\n",
            " 'micheal cranford' 'mikec' 'mikec sail' 'minsky' 'minsky an'\n",
            " 'minsky media' 'mit' 'mit edu' 'monash' 'monash edu' 'monu6' 'monu6 cc'\n",
            " 'mrc' 'mrc tomobiki' 'my' 'my kill' 'my save' 'my sole' 'nazareth'\n",
            " 'nazareth from' 'nazareth israel' 'nntp' 'nntp posting' 'no' 'no notion'\n",
            " 'not' 'not for' 'notables' 'notables from' 'note' 'note the' 'noted'\n",
            " 'noted by' 'noted the' 'nothing' 'notion' 'notion at' 'notion of' 'now'\n",
            " 'now organization' 'nyx' 'nyx cs' 'of' 'of about' 'of fear'\n",
            " 'of geocentrism' 'of heliocentric' 'of nazareth' 'of reason' 'of this'\n",
            " 'on' 'on 19' 'on the' 'one' 'one and' 'only' 'only get' 'only subject'\n",
            " 'or' 'or anywhere' 'or even' 'or lines' 'organization'\n",
            " 'organization case' 'organization salvation' 'organization tektronix'\n",
            " 'others' 'others have' 'own' 'own curse' 'package' 'package note' 'part'\n",
            " 'part sting' 'past' 'past things' 'pepper' 'pepper east' 'perry'\n",
            " 'perry apollo' 'perry from' 'petrich' 'petrich from' 'place' 'place in'\n",
            " 'plays' 'plays his' 'po' 'po cwru' 'positive' 'positive aspect' 'posting'\n",
            " 'posting host' 'posting provoked' 'posting rate' 'posting saved'\n",
            " 'posting to' 'posts' 'posts the' 'predictions' 'predictions was'\n",
            " 'provoked' 'provoked me' 'question' 'question was' 'qur' 'qur an' 'rate'\n",
            " 'rate is' 're' 're atheist' 're genocide' 're islam' 're where' 'reached'\n",
            " 'reached at' 'reason' 'reason every' 'rel' 'rel jesus' 'reserve'\n",
            " 'reserve university' 'rice' 'rice writes' 'ryan' 'ryan subject' 'sail'\n",
            " 'sail labs' 'salvation' 'salvation army' 'save' 'save file' 'saved'\n",
            " 'saved in' 'scientific' 'scientific predictions' 'seen' 'seen posting'\n",
            " 'software' 'software package' 'sole' 'sole intention' 'sting' 'sting his'\n",
            " 'student' 'student cwru' 'subject' 'subject disney' 'subject re' 'such'\n",
            " 'such that' 'sun' 'sun com' 'sunlight' 'sunlight llnl' 'teach' 'teach us'\n",
            " 'tek' 'tek com' 'tektronix' 'tektronix inc' 'tell' 'tell me' 'that'\n",
            " 'that my' 'that was' 'that while' 'the' 'the actor' 'the commonly'\n",
            " 'the current' 'the disney' 'the early' 'the first' 'the following'\n",
            " 'the followup' 'the last' 'the one' 'the past' 'the positive'\n",
            " 'the posting' 'the qur' 'the subject' 'the time' 'the voice' 'their'\n",
            " 'their way' 'there' 'there is' 'they' 'they now' 'things' 'things can'\n",
            " 'think' 'think he' 'this' 'this is' 'this verse' 'three' 'three years'\n",
            " 'time' 'time afterwards' 'time and' 'to' 'to alt' 'to fly' 'to place'\n",
            " 'tomobiki' 'tomobiki cho' 'too' 'too that' 'tx' 'tx us' 'uavax0'\n",
            " 'uavax0 ccit' 'university' 'university lines' 'us' 'us think'\n",
            " 'us without' 'verse' 'verse noted' 'verse or' 'voice' 'voice of' 'waco'\n",
            " 'waco tx' 'was' 'was article' 'was by' 'was last' 'was learning' 'was re'\n",
            " 'was the' 'washington' 'washington edu' 'way' 'way to' 'were'\n",
            " 'were articles' 'western' 'western reserve' 'where' 'where are'\n",
            " 'where is' 'which' 'which is' 'while' 'while geocentrism' 'will'\n",
            " 'will find' 'will teach' 'without' 'without freedom' 'without the'\n",
            " 'wonder' 'wonder where' 'words' 'words of' 'worse' 'worse nothing'\n",
            " 'writes' 'writes the' 'writes your' 'years' 'years as' 'you' 'you hear'\n",
            " 'your' 'your heart' 'your posting' 'your question' 'yoyo' 'yoyo cc']\n"
          ]
        }
      ],
      "source": [
        "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(1, 2))\n",
        "\n",
        "X2 = vectorizer2.fit_transform(twenty_train.data[:3])\n",
        "print(vectorizer2.get_feature_names_out())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWvDUR9DxL4y",
        "outputId": "d7071880-e8df-4110-c279-2902aa68c6a6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
              "        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2,\n",
              "        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
              "        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,\n",
              "        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "X2[0].toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJBhQf02P0Cm",
        "outputId": "cfb14692-9da4-4af9-f0ac-36023962f925"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 229)\t1\n",
            "  (0, 374)\t1\n",
            "  (0, 499)\t1\n",
            "  (0, 331)\t1\n",
            "  (0, 530)\t1\n",
            "  (0, 136)\t1\n",
            "  (0, 372)\t1\n",
            "  (0, 145)\t1\n",
            "  (0, 519)\t1\n",
            "  (0, 171)\t2\n",
            "  (0, 49)\t2\n",
            "  (0, 435)\t1\n",
            "  (0, 532)\t1\n",
            "  (0, 287)\t1\n",
            "  (0, 95)\t1\n",
            "  (0, 431)\t1\n",
            "  (0, 342)\t1\n",
            "  (0, 113)\t1\n",
            "  (0, 52)\t1\n",
            "  (0, 534)\t1\n",
            "  (0, 362)\t2\n",
            "  (0, 54)\t1\n",
            "  (0, 24)\t1\n",
            "  (0, 540)\t2\n",
            "  (0, 511)\t1\n",
            "  :\t:\n",
            "  (2, 561)\t2\n",
            "  (2, 302)\t2\n",
            "  (2, 397)\t2\n",
            "  (2, 410)\t2\n",
            "  (2, 418)\t1\n",
            "  (2, 245)\t1\n",
            "  (2, 285)\t1\n",
            "  (2, 595)\t1\n",
            "  (2, 432)\t1\n",
            "  (2, 57)\t1\n",
            "  (2, 554)\t1\n",
            "  (2, 479)\t1\n",
            "  (2, 43)\t1\n",
            "  (2, 419)\t1\n",
            "  (2, 267)\t1\n",
            "  (2, 433)\t1\n",
            "  (2, 197)\t1\n",
            "  (2, 241)\t1\n",
            "  (2, 195)\t1\n",
            "  (2, 390)\t1\n",
            "  (2, 514)\t1\n",
            "  (2, 290)\t1\n",
            "  (2, 604)\t1\n",
            "  (2, 339)\t1\n",
            "  (2, 578)\t1\n"
          ]
        }
      ],
      "source": [
        "print(X2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmFsj1B0P0Cp",
        "outputId": "a4813188-f06e-4d38-94e9-62265d1462c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 1 ... 1 0 0]\n",
            " [1 1 0 ... 0 1 1]]\n"
          ]
        }
      ],
      "source": [
        "print(X2.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGwbCzhxP0Cs"
      },
      "source": [
        "Recap\n",
        "\n",
        "we learned the  main operations in an NLP problem:\n",
        "\n",
        ". \n",
        "\n",
        "It's also possible to correct words using packages like autocorrect."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hq_OXgAh-6F3"
      },
      "source": [
        "## Excercise\n",
        "\n",
        "Apply the transformations we just saw to thw fake-news dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wx_duPggP0Cx"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "# api.info()  # return dict with info about available models/datasets\n",
        "api.info('fake-news')\n",
        "\n",
        "# api.info(\"text8\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vqg6GumnP0C0"
      },
      "outputs": [],
      "source": [
        "fn = api.load('fake-news')\n",
        "for f in fn:\n",
        "  print(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lz7B4CC3CPYh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "lfn = pd.DataFrame(fn)\n",
        "\n",
        "lfn.spam_score.astype(float).hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7w1g9Ign0otF"
      },
      "outputs": [],
      "source": [
        "lfn[:1].text. # complete here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZtzCC-tDBwl"
      },
      "outputs": [],
      "source": [
        "f = lfn[:30].text.apply(word_tokenize).apply(lambda x: [i for i in x if i not in stopwords_list])\n",
        "# apply(lambda i: set(list(i)) - stopwords)\n",
        "from gensim.parsing.porter import PorterStemmer\n",
        "p = # complete here\n",
        "\n",
        "f.apply(p.stem_documents).apply(nltk.# complete here)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMLhyAjs0onm"
      },
      "outputs": [],
      "source": [
        "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
        "\n",
        "X2 = vectorizer2  # complete here\n",
        "print(vectorizer2.get_feature_names())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWHSl1JvWQpe"
      },
      "source": [
        "Now we can write the spam_score into a file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGZSlK8PP0C2"
      },
      "outputs": [],
      "source": [
        "import io \n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "# write the file\n",
        "\n",
        " "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2932607960ea4f1a8a338ab12861703e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ccea2c60e80e442aa07e83d5485bdc56",
              "IPY_MODEL_5d89a66716114c01a9b429c96741f603",
              "IPY_MODEL_f2f007da9cee40d69d160d76b522833a"
            ],
            "layout": "IPY_MODEL_a846fe3e3cd04da1b99b56ff1773c03d"
          }
        },
        "ccea2c60e80e442aa07e83d5485bdc56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c3af867bf1741c49d7ade46ec8a7a0e",
            "placeholder": "​",
            "style": "IPY_MODEL_47b388c358534bd9843ca7074bf1a717",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "5d89a66716114c01a9b429c96741f603": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cadf6bd60ab04b44afe6678dd1cf6eb0",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3ef640bc887b4d9d9481d378a3a6a83f",
            "value": 231508
          }
        },
        "f2f007da9cee40d69d160d76b522833a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a96fcfccc56d4b5bb05619ad8243e335",
            "placeholder": "​",
            "style": "IPY_MODEL_d4d1b5fc4f2f4c4b9735bed277cefc6a",
            "value": " 232k/232k [00:00&lt;00:00, 3.65MB/s]"
          }
        },
        "a846fe3e3cd04da1b99b56ff1773c03d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c3af867bf1741c49d7ade46ec8a7a0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47b388c358534bd9843ca7074bf1a717": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cadf6bd60ab04b44afe6678dd1cf6eb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ef640bc887b4d9d9481d378a3a6a83f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a96fcfccc56d4b5bb05619ad8243e335": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4d1b5fc4f2f4c4b9735bed277cefc6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "abbc12b8bdde487f835efa390912adb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a629011446a34c90912e62b0a013ce59",
              "IPY_MODEL_45c335466077453aa653a501c8cf60b4",
              "IPY_MODEL_cc57db1bc1af4799b314a79d074efd31"
            ],
            "layout": "IPY_MODEL_c0a752aa4a774813b0f87997f9435438"
          }
        },
        "a629011446a34c90912e62b0a013ce59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54e33040062b4f60a8ee7a4cbc122f71",
            "placeholder": "​",
            "style": "IPY_MODEL_5345afd8a35a4fd6a25869c8579d743f",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "45c335466077453aa653a501c8cf60b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4519c9e909024ef392402368c2031ce6",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_152d1b26614d454b8737a21ff82ca82c",
            "value": 28
          }
        },
        "cc57db1bc1af4799b314a79d074efd31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8bec05a0fef4730a4453eb870cb20c4",
            "placeholder": "​",
            "style": "IPY_MODEL_478dc353ac6f43128448c85f85f5ab6a",
            "value": " 28.0/28.0 [00:00&lt;00:00, 1.43kB/s]"
          }
        },
        "c0a752aa4a774813b0f87997f9435438": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54e33040062b4f60a8ee7a4cbc122f71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5345afd8a35a4fd6a25869c8579d743f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4519c9e909024ef392402368c2031ce6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "152d1b26614d454b8737a21ff82ca82c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b8bec05a0fef4730a4453eb870cb20c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "478dc353ac6f43128448c85f85f5ab6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "31650d3af55e430a806ea22a78056af3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9d203f2e2b3a4e919d986b5d28e784ba",
              "IPY_MODEL_0d13073921844f8baabc3c04e2b1ba70",
              "IPY_MODEL_7f02b2d45b4b432f82e36c20da00312e"
            ],
            "layout": "IPY_MODEL_81bfc151ac614dcd934e47d53bed30e7"
          }
        },
        "9d203f2e2b3a4e919d986b5d28e784ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ac56932326c47e18f5f7385fa46e4a3",
            "placeholder": "​",
            "style": "IPY_MODEL_3f596961bede4f04876517fc9e0b28c6",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "0d13073921844f8baabc3c04e2b1ba70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a162b99287b741788ac20c13d445d10a",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6579386b7d5a44b7b161c44a78edf5ab",
            "value": 570
          }
        },
        "7f02b2d45b4b432f82e36c20da00312e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef9f5a2e04be4f45bd36de37512b16fe",
            "placeholder": "​",
            "style": "IPY_MODEL_33514b95ca8046f2a65a0101b46c8881",
            "value": " 570/570 [00:00&lt;00:00, 26.2kB/s]"
          }
        },
        "81bfc151ac614dcd934e47d53bed30e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ac56932326c47e18f5f7385fa46e4a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f596961bede4f04876517fc9e0b28c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a162b99287b741788ac20c13d445d10a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6579386b7d5a44b7b161c44a78edf5ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ef9f5a2e04be4f45bd36de37512b16fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33514b95ca8046f2a65a0101b46c8881": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}